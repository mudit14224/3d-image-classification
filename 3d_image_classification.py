# -*- coding: utf-8 -*-
"""3d-image-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W-UyPgfjctHjO9kG2aao30j35rcUECT2
"""

# Imports
import os
import zipfile
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import get_file
import nibabel as nib
from scipy import ndimage
import random
import matplotlib.pyplot as plt

# Download the data
# Download the normal CT scans
url = "https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-0.zip"
filename = os.path.join(os.getcwd(), "CT-0.zip")
get_file(filename, url)

# Download the abnormal CT scans
url = "https://github.com/hasibzunair/3D-image-classification-tutorial/releases/download/v0.2/CT-23.zip"
filename = os.path.join(os.getcwd(), "CT-23.zip")
get_file(filename, url)

os.makedirs("MedData")

# Unzip the zip files in the MedData folder
with zipfile.ZipFile("CT-0.zip", "r") as z:
  z.extractall("./MedData/")

with zipfile.ZipFile("CT-23.zip", "r") as z:
  z.extractall("./MedData/")

# Loading the data and Preprocessing 

def read_nifti(filename):
  scan = nib.load(filename)
  scan = scan.get_fdata()
  return scan

def normalize(volume):
  min=-1000
  max=400
  volume[volume<min] = min
  volume[volume>max] = max
  volume = (volume - min)/(max - min)
  volume = volume.astype("float32")
  return volume

def resize_volume(img):
  d_depth = 64
  d_height = 128
  d_width = 128
  c_depth = img.shape[-1]
  c_height = img.shape[1]
  c_width = img.shape[0]

  depth = c_depth/d_depth
  height = c_height/d_height
  width = c_width/d_width

  depth_factor = 1/depth
  height_factor = 1/height
  width_factor = 1/width

  img = ndimage.rotate(img, 90, reshape=False)
  # Resize along the z axis
  img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)

  return img

def process_scan(path):
  volume = read_nifti(path)
  volume = normalize(volume)
  volume = resize_volume(volume)
  return volume

normal_scan_path = [
      os.path.join(os.getcwd(), "./MedData/CT-0", x) for x in os.listdir("./MedData/CT-0")
]

abnormal_scan_path = [
      os.path.join(os.getcwd(), "./MedData/CT-23", x) for x in os.listdir("./MedData/CT-23")
]

print("Number of normal ct scans: " + str(len(normal_scan_path)))
print("Number of abnormal ct scans: "+ str(len(abnormal_scan_path)))

# Build train and validation datasets
abnormal_scans = np.array([process_scan(path) for path in abnormal_scan_path])
normal_scans = np.array([process_scan(path) for path in normal_scan_path])

abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])
normal_labels = np.array([0 for _ in range(len(normal_scans))])

X_train = np.concatenate((abnormal_scans[:70], normal_scans[:70]), axis=0)
y_train = np.concatenate((abnormal_labels[:70], normal_labels[:70]), axis=0)
X_test = np.concatenate((abnormal_scans[70:], normal_scans[70:]), axis=0)
y_test = np.concatenate((abnormal_labels[70:], normal_labels[70:]), axis=0)
print("Number of samples in train and validation are: {} and {}".format(X_train.shape[0], X_test.shape[0]))

# Data Augmentation
@tf.function
def rotate(volume):
  def scipy_rotate(volume):
    angles = [-20, -10, -5, 5, 10, 20]
    angle = random.choice(angles)
    volume = ndimage.rotate(volume, angle, reshape=False)
    volume[volume < 0] = 0
    volume[volume > 1] = 1
    return volume
  
  augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)
  return augmented_volume

def train_preprocessing(volume, label):
  volume = rotate(volume)
  volume = tf.expand_dims(volume, axis=3)
  return volume, label

def validation_preprocessing(volume, label):
  volume = tf.expand_dims(volume, axis=3)
  return volume, label

train_loader = tf.data.Dataset.from_tensor_slices((X_train, y_train))
test_loader = tf.data.Dataset.from_tensor_slices((X_test, y_test))

batch_size = 2

train_dataset = (
    train_loader.shuffle(len(X_train)).map(train_preprocessing).batch(batch_size).prefetch(2)
)

test_dataset = (
    test_loader.shuffle(len(X_test)).map(validation_preprocessing).batch(batch_size).prefetch(2)
)

# Lets Visualize a Ct scan
data = train_dataset.take(1)
images, labels = list(data)[0]
images = images.numpy()
image = images[0]
print("Dimension of the CT scan is:", image.shape)
plt.imshow(np.squeeze(image[:, :, 30]), cmap="gray")

def plot_slices(num_rows, num_columns, width, height, data):
  data = np.rot90(np.array(data))
  data = np.transpose(data)
  data = np.reshape(data, (num_rows, num_columns, width, height))
  rows_data, columns_data = data.shape[0], data.shape[1]
  heights = [slc[0].shape[0] for slc in data]
  widths = [slc[0].shape[1] for slc in data]
  fig_width = 12.0
  fig_height = fig_width * sum(heights) / sum(widths)
  f, ax = plt.subplots(
      rows_data, 
      columns_data, 
      figsize=(fig_width, fig_height),
      gridspec_kw={"height_ratios": heights},
  )
  for i in range(rows_data):
    for j in range(columns_data):
      ax[i, j].imshow(data[i][j], cmap="gray")
      ax[i, j].axis("off")
  plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)
  plt.show()

# 4 rows and 10 columns for 100 slices of the CT scan.
plot_slices(4, 10, 128, 128, image[:, :, :40])

# Define the 3D CNN model
def get_model(width=128, height=128, depth=64):
  inputs = keras.Input((width, height, depth, 1))
  
  x = layers.Conv3D(filters=64, kernel_size=3, activation="relu")(inputs)
  x = layers.MaxPool3D(pool_size=2)(x)
  x = layers.BatchNormalization()(x)

  x = layers.Conv3D(filters=64, kernel_size=3, activation="relu")(x)
  x = layers.MaxPool3D(pool_size=2)(x)
  x = layers.BatchNormalization()(x)

  x = layers.Conv3D(filters=128, kernel_size=3, activation="relu")(x)
  x = layers.MaxPool3D(pool_size=2)(x)
  x = layers.BatchNormalization()(x)

  x = layers.Conv3D(filters=256, kernel_size=3, activation="relu")(x)
  x = layers.MaxPool3D(pool_size=2)(x)
  x = layers.BatchNormalization()(x)

  x = layers.GlobalAveragePooling3D()(x)
  x = layers.Dense(units=512, activation="relu")(x)
  x = layers.Dropout(0.3)(x)

  outputs = layers.Dense(units=1, activation="sigmoid")(x)

  # Define the model
  model = keras.Model(inputs, outputs, name="3DCNN")
  return model

model = get_model()
model.summary()

# Train the model
lr = 0.0001
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    lr, decay_steps=100000, decay_rate=0.96, staircase=True
)
model.compile(loss="binary_crossentropy", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=['acc'])

# Define the callbacks
checkpoint_cb = keras.callbacks.ModelCheckpoint("3d-img-class.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(monitor="val_acc", patience=15)

# Train the model
epochs=100
model.fit(train_dataset, validation_data=test_dataset, epochs=epochs, shuffle=True, verbose=2, callbacks=[checkpoint_cb, early_stopping_cb])

# Visualize the model performance
fig, ax = plt.subplots(1, 2, figsize=(20, 3))
ax = ax.ravel()

for i, metric in enumerate(["acc", "loss"]):
  ax[i].plot(model.history.history[metric])
  ax[i].plot(model.history.history["val_" + metric])
  ax[i].set_title("Model {}".format(metric))
  ax[i].set_xlabel("epochs")
  ax[i].set_ylabel(metric)
  ax[i].legend(["train", "val"])

# Make Predictions using CT scan
model.load_weights("3d-img-class.h5")
prediction = model.predict(np.expand_dims(X_test[0], axis=0))[0]
scores = [1 - prediction[0], prediction[0]]
class_names = ["normal", "abnormal"]
for score, name in zip(scores, class_names):
  print("This model is %.2f percent confident that CT scan is %s"% ((100 * score), name))